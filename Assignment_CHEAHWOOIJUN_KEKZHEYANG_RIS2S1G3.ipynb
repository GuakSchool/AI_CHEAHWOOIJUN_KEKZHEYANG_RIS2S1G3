{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install streamlit\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_Il0RBwudCv",
        "outputId": "09d99c02-a4c3-4510-d7cb-363ae7028aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.49.1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl5QRonUMqAW"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "import requests\n",
        "from io import StringIO\n",
        "import os\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load data from Google Drive\n",
        "def load_data_from_drive():\n",
        "    movies_path = '/content/drive/MyDrive/AI/movies_metadata.csv'\n",
        "    credits_path = '/content/drive/MyDrive/AI/credits.csv'\n",
        "    ratings_path = '/content/drive/MyDrive/AI/ratings_small.csv'\n",
        "\n",
        "    movies = pd.read_csv(movies_path)\n",
        "    credits = pd.read_csv(credits_path)\n",
        "    ratings = pd.read_csv(ratings_path)\n",
        "\n",
        "    # Filter movies with more votes for faster processing\n",
        "    movies = movies[movies['vote_count'] > 500]\n",
        "\n",
        " # Convert IDs\n",
        "    movies['id'] = pd.to_numeric(movies['id'], errors='coerce')\n",
        "    credits['id'] = pd.to_numeric(credits['id'], errors='coerce')\n",
        "    ratings['movieId'] = pd.to_numeric(ratings['movieId'], errors='coerce')\n",
        "    movies = movies.dropna(subset=['id'])\n",
        "    credits = credits.dropna(subset=['id'])\n",
        "    ratings = ratings.dropna(subset=['movieId'])\n",
        "    movies['id'] = movies['id'].astype(int)\n",
        "    credits['id'] = credits['id'].astype(int)\n",
        "    ratings['movieId'] = ratings['movieId'].astype(int)\n",
        "\n",
        "    # Merge movies + credits\n",
        "    movies = movies.merge(credits, on='id', how='inner')\n",
        "\n",
        "    # Clean features\n",
        "    movies['overview'] = movies['overview'].fillna('')\n",
        "    movies['tagline'] = movies['tagline'].fillna('')\n",
        "    movies['description'] = movies['overview'] + \" \" + movies['tagline']\n",
        "\n",
        "    movies = movies[['id', 'title', 'description', 'genres', 'cast', 'crew']]\n",
        "\n",
        "    # Parsing helpers\n",
        "    def parse_genres(obj):\n",
        "        try: return [i['name'] for i in ast.literal_eval(obj)]\n",
        "        except: return []\n",
        "    def parse_cast(obj):\n",
        "        try: return [i['name'] for i in ast.literal_eval(obj)[:3]]\n",
        "        except: return []\n",
        "    def parse_crew(obj):\n",
        "        try: return [i['name'] for i in ast.literal_eval(obj) if i['job']=='Director']\n",
        "        except: return []\n",
        "\n",
        "    movies['genres'] = movies['genres'].apply(parse_genres)\n",
        "    movies['cast'] = movies['cast'].apply(parse_cast)\n",
        "    movies['crew'] = movies['crew'].apply(parse_crew)\n",
        "\n",
        "    # Convert lists → strings\n",
        "    movies['genres'] = movies['genres'].apply(lambda x: \" \".join(x))\n",
        "    movies['cast'] = movies['cast'].apply(lambda x: \" \".join(x))\n",
        "    movies['crew'] = movies['crew'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    movies['final_features'] = (\n",
        "        movies['description'] + ' ' +\n",
        "        movies['genres'] + ' ' +\n",
        "        movies['cast'] + ' ' +\n",
        "        movies['crew']\n",
        "    )\n",
        "    return movies, ratings"
      ],
      "metadata": {
        "id": "ZfMQmix9u-Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit page config\n",
        "st.set_page_config(\n",
        "    page_title=\"Movie Recommender System\",\n",
        "    page_icon=\"🎬\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "MOVIES_URL = \"https://drive.google.com/uc?export=download&id=1GOuUEu1-KgepbjTxIOkbAU8VNJ5lfEg3\"\n",
        "CREDITS_URL = \"https://drive.google.com/uc?export=download&id=10iuK9C87fYLyDLJhqT3bpVv1A2IErmHR\"\n",
        "RATINGS_URL = \"https://drive.google.com/uc?export=download&id=122XJoryYXvv3AUa6F_y1KiCcYdXQjEp4\"\n",
        "\n",
        "@st.cache_data\n",
        "def load_data_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return pd.read_csv(StringIO(response.text))\n",
        "\n",
        "@st.cache_resource\n",
        "def load_data():\n",
        "    with st.spinner(\"Loading datasets...\"):\n",
        "        movies = load_data_from_url(MOVIES_URL)\n",
        "        credits = load_data_from_url(CREDITS_URL)\n",
        "        ratings = load_data_from_url(RATINGS_URL)\n",
        "\n",
        "    # Filter movies with more votes for faster dev\n",
        "    movies = movies[movies['vote_count'] > 500]\n",
        "\n",
        "    # Convert IDs\n",
        "    movies['id'] = pd.to_numeric(movies['id'], errors='coerce')\n",
        "    credits['id'] = pd.to_numeric(credits['id'], errors='coerce')\n",
        "    ratings['movieId'] = pd.to_numeric(ratings['movieId'], errors='coerce')\n",
        "    movies = movies.dropna(subset=['id'])\n",
        "    credits = credits.dropna(subset=['id'])\n",
        "    ratings = ratings.dropna(subset=['movieId'])\n",
        "    movies['id'] = movies['id'].astype(int)\n",
        "    credits['id'] = credits['id'].astype(int)\n",
        "    ratings['movieId'] = ratings['movieId'].astype(int)\n",
        "\n",
        "    # Merge movies + credits\n",
        "    movies = movies.merge(credits, on='id', how='inner')\n",
        "\n",
        "    # Clean features\n",
        "    movies['overview'] = movies['overview'].fillna('')\n",
        "    movies['tagline'] = movies['tagline'].fillna('')\n",
        "    movies['description'] = movies['overview'] + \" \" + movies['tagline']\n",
        "\n",
        "    movies = movies[['id', 'title', 'description', 'genres', 'cast', 'crew']]\n",
        "\n",
        "    # Parsing helpers\n",
        "    def parse_genres(obj):\n",
        "        try: return [i['name'] for i in ast.literal_eval(obj)]\n",
        "        except: return []\n",
        "    def parse_cast(obj):\n",
        "        try: return [i['name'] for i in ast.literal_eval(obj)[:3]]\n",
        "        except: return []\n",
        "    def parse_crew(obj):\n",
        "        try: return [i['name'] for i in ast.literal_eval(obj) if i['job']=='Director']\n",
        "        except: return []\n",
        "\n",
        "    movies['genres'] = movies['genres'].apply(parse_genres)\n",
        "    movies['cast'] = movies['cast'].apply(parse_cast)\n",
        "    movies['crew'] = movies['crew'].apply(parse_crew)\n",
        "\n",
        "    # Convert lists → strings\n",
        "    movies['genres'] = movies['genres'].apply(lambda x: \" \".join(x))\n",
        "    movies['cast'] = movies['cast'].apply(lambda x: \" \".join(x))\n",
        "    movies['crew'] = movies['crew'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    movies['final_features'] = (\n",
        "        movies['description'] + ' ' +\n",
        "        movies['genres'] + ' ' +\n",
        "        movies['cast'] + ' ' +\n",
        "        movies['crew']\n",
        "    )\n",
        "\n",
        "    return movies, ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tYo0Ct9ruYS",
        "outputId": "397c597f-97f1-493b-c2da-777e5b6d6bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-07 15:25:15.043 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:25:15.049 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Create TF-IDF model\n",
        "@st.cache_resource\n",
        "def create_tfidf_model(movies, pickle_path=\"/content/drive/MyDrive/tfidf_model.pkl\"):\n",
        "    # If pickle exists, load it\n",
        "    try:\n",
        "        with open(pickle_path, \"rb\") as f:\n",
        "            tfidf, vectors = pickle.load(f)\n",
        "        print(\"Loaded TF-IDF model from pickle\")\n",
        "    except:\n",
        "        print(\"Creating new TF-IDF model...\")\n",
        "        tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "        vectors = tfidf.fit_transform(movies['final_features'])\n",
        "        # Save to pickle\n",
        "        with open(pickle_path, \"wb\") as f:\n",
        "            pickle.dump((tfidf, vectors), f)\n",
        "        print(\"TF-IDF model created and saved\")\n",
        "    return tfidf, vectors\n"
      ],
      "metadata": {
        "id": "BwFVLmo8sFTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Prepare collaborative data\n",
        "@st.cache_resource\n",
        "def prepare_collaborative_data(movies, ratings):\n",
        "    movies_cf = movies[['id','title']].rename(columns={'id':'movieId'})\n",
        "    ratings = ratings.merge(movies_cf, on=\"movieId\", how=\"inner\")\n",
        "\n",
        "    user_mapping = {1: \"Bob\", 2: \"Alice\", 3: \"Charlie\", 4: \"Diana\", 5: \"Eve\"}\n",
        "    ratings['user_name'] = ratings['userId'].replace(user_mapping)\n",
        "\n",
        "    user_item_matrix = ratings.pivot_table(index='user_name', columns='title', values='rating').fillna(0)\n",
        "\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    user_sim = cosine_similarity(user_item_matrix)\n",
        "    user_sim_df = pd.DataFrame(user_sim, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
        "\n",
        "    return ratings, user_item_matrix, user_sim_df\n"
      ],
      "metadata": {
        "id": "jmf0DnRmvNAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Recommendation functions\n",
        "# Content-based\n",
        "def content_based_recommend(movie_title, movies, vectors, top_n=10):\n",
        "    if movie_title not in movies['title'].values:\n",
        "        return \"❌ Movie not found\", []\n",
        "    idx = movies[movies['title']==movie_title].index[0]\n",
        "    scores = linear_kernel(vectors[idx], vectors).flatten()\n",
        "    indices = scores.argsort()[-(top_n+1):-1][::-1]\n",
        "    return \"Content-based recommendations\", [(movies.iloc[i].title, float(scores[i])) for i in indices]\n",
        "\n",
        "# Collaborative\n",
        "def collaborative_recommend(user_name, user_item_matrix, user_sim_df, top_n=50):\n",
        "    if user_name not in user_item_matrix.index:\n",
        "        return {}\n",
        "    sim_scores = user_sim_df[user_name].drop(user_name).sort_values(ascending=False)\n",
        "    top_users = sim_scores.index[:5]\n",
        "    neighbor_ratings = user_item_matrix.loc[top_users].mean(axis=0)\n",
        "    watched = user_item_matrix.loc[user_name][user_item_matrix.loc[user_name]>0].index\n",
        "    neighbor_ratings = neighbor_ratings.drop(watched, errors='ignore')\n",
        "    top_recs = neighbor_ratings.sort_values(ascending=False).head(top_n)\n",
        "    return {title: score for title, score in top_recs.items()}\n",
        "\n",
        "# Hybrid\n",
        "def hybrid_recommend(user_name, liked_movie, movies, vectors, user_item_matrix, user_sim_df, alpha=0.5, top_n=10):\n",
        "    if not user_name or user_name.strip()==\"\" or user_name==\"-\":\n",
        "        user_name = f\"User_{random.randint(6,671)}\"\n",
        "    collab_scores = collaborative_recommend(user_name, user_item_matrix, user_sim_df, top_n=50)\n",
        "    if liked_movie not in movies['title'].values:\n",
        "        return user_name, [(\"❌ Movie not found\", 0.0)]\n",
        "    idx = movies.index[movies['title']==liked_movie][0]\n",
        "    cs = linear_kernel(vectors[idx], vectors).flatten()\n",
        "    content_scores = {movies.iloc[i].title: float(cs[i]) for i in cs.argsort()[-51:-1]}\n",
        "    all_titles = set(collab_scores.keys()) | set(content_scores.keys())\n",
        "    hybrid_scores = {t: alpha*content_scores.get(t,0)+ (1-alpha)*collab_scores.get(t,0) for t in all_titles}\n",
        "    ranked = sorted(hybrid_scores.items(), key=lambda x:x[1], reverse=True)[:top_n]\n",
        "    return user_name, [(t,float(s)) for t,s in ranked]\n"
      ],
      "metadata": {
        "id": "DDwlaqHzsMdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Evaluation Functions -----\n",
        "def content_predict(user_id, movie_id, movies, ratings_aggregated, content_similarity):\n",
        "    if movie_id not in movies['id'].values:\n",
        "        return None\n",
        "    idx = movies[movies['id'] == movie_id].index[0]\n",
        "    sims = content_similarity[idx]\n",
        "    user_ratings = ratings_aggregated[ratings_aggregated['userId'] == user_id]\n",
        "    if user_ratings.empty:\n",
        "        return None\n",
        "    sim_scores = []\n",
        "    for _, row in user_ratings.iterrows():\n",
        "        if row['movieId'] in movies['id'].values:\n",
        "            jdx = movies[movies['id'] == row['movieId']].index[0]\n",
        "            sim_scores.append((row['rating'], sims[jdx]))\n",
        "    if not sim_scores:\n",
        "        return None\n",
        "    weighted_sum = sum(r*s for r,s in sim_scores)\n",
        "    sim_sum = sum(s for _,s in sim_scores)\n",
        "    return weighted_sum/sim_sum if sim_sum != 0 else None\n",
        "\n",
        "# Aggregate duplicate ratings by taking the mean before pivoting\n",
        "# This is now done outside the function\n",
        "\n",
        "def collab_predict(user_id, movie_id, ratings_matrix, user_sim_df):\n",
        "    if movie_id not in ratings_matrix.columns:\n",
        "        return None\n",
        "    if user_id not in user_sim_df.index:\n",
        "        return None\n",
        "    sims = user_sim_df[user_id].drop(user_id, errors='ignore')\n",
        "    top_users = sims.sort_values(ascending=False).head(5).index\n",
        "\n",
        "    # Check if top_users is empty\n",
        "    if top_users.empty:\n",
        "        return None\n",
        "\n",
        "    # Ensure movie_id is in the columns of ratings_matrix for top_users\n",
        "    if movie_id not in ratings_matrix.columns:\n",
        "        return None\n",
        "\n",
        "    top_ratings = ratings_matrix.loc[top_users, movie_id]\n",
        "    weights = sims.loc[top_users]\n",
        "\n",
        "    # Handle cases where weights might be all zeros\n",
        "    if weights.sum() == 0:\n",
        "        return None\n",
        "\n",
        "    return np.dot(top_ratings, weights)/weights.sum()\n",
        "\n",
        "def hybrid_predict(user_id, movie_id, movies, ratings_aggregated, content_similarity, ratings_matrix, user_sim_df, alpha=0.5):\n",
        "    cp = content_predict(user_id, movie_id, movies, ratings_aggregated, content_similarity)\n",
        "    cf = collab_predict(user_id, movie_id, ratings_matrix, user_sim_df)\n",
        "    if cp is None and cf is None:\n",
        "        return None\n",
        "    if cp is None: return cf\n",
        "    if cf is None: return cp\n",
        "    return alpha*cp + (1-alpha)*cf\n",
        "\n",
        "def evaluate_model(predict_func, movies, ratings_aggregated, content_similarity, ratings_matrix, user_sim_df, n_samples=300):\n",
        "    # Ensure the sampled ratings are in the aggregated dataframe\n",
        "    test = ratings_aggregated.sample(n_samples, random_state=42)\n",
        "    preds, truths = [], []\n",
        "    for _, row in test.iterrows():\n",
        "        # Pass necessary arguments to predict_func\n",
        "        if predict_func.__name__ == 'content_predict':\n",
        "            pred = predict_func(row['userId'], row['movieId'], movies, ratings_aggregated, content_similarity)\n",
        "        elif predict_func.__name__ == 'collab_predict':\n",
        "             pred = predict_func(row['userId'], row['movieId'], ratings_matrix, user_sim_df)\n",
        "        elif predict_func.__name__ == 'hybrid_predict':\n",
        "             pred = predict_func(row['userId'], row['movieId'], movies, ratings_aggregated, content_similarity, ratings_matrix, user_sim_df)\n",
        "        else:\n",
        "            pred = predict_func(row['userId'], row['movieId']) # For other predict functions\n",
        "\n",
        "        if pred is not None:\n",
        "            preds.append(pred)\n",
        "            truths.append(row['rating'])\n",
        "    if not preds:\n",
        "        return None, None\n",
        "    mse = mean_squared_error(truths, preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return mse, rmse\n",
        "\n",
        "# ----- Run Evaluation -----\n",
        "# This part will be moved to the next cell (Cell 7) to ensure data loading and TF-IDF creation are done first."
      ],
      "metadata": {
        "id": "MKKoRN2VsT5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Run evaluation and Demo recommendations\n",
        "\n",
        "# Load data and create TF-IDF model\n",
        "movies, ratings = load_data_from_drive()\n",
        "tfidf, vectors = create_tfidf_model(movies)\n",
        "\n",
        "# Prepare collaborative data\n",
        "ratings_aggregated = ratings.groupby(['userId', 'movieId'])['rating'].mean().reset_index()\n",
        "ratings_matrix = ratings_aggregated.pivot(index=\"userId\", columns=\"movieId\", values=\"rating\").fillna(0)\n",
        "user_sim = cosine_similarity(ratings_matrix)\n",
        "user_sim_df_eval = pd.DataFrame(user_sim, index=ratings_matrix.index, columns=ratings_matrix.index) # Renamed to avoid conflict with user_sim_df in hybrid_recommend\n",
        "content_similarity = linear_kernel(vectors, vectors) # Calculate content similarity matrix\n",
        "\n",
        "print(\"Running evaluation...\")\n",
        "mse_content, rmse_content = evaluate_model(content_predict, movies, ratings_aggregated, content_similarity, ratings_matrix, user_sim_df_eval)\n",
        "mse_collab, rmse_collab = evaluate_model(collab_predict, movies, ratings_aggregated, content_similarity, ratings_matrix, user_sim_df_eval) # Removed movies, content_similarity as they are not used in collab_predict within evaluate_model\n",
        "mse_hybrid, rmse_hybrid = evaluate_model(hybrid_predict, movies, ratings_aggregated, content_similarity, ratings_matrix, user_sim_df_eval)\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    \"System\": [\"Content-Based\", \"Collaborative\", \"Hybrid\"],\n",
        "    \"MSE\": [mse_content, mse_collab, mse_hybrid],\n",
        "    \"RMSE\": [rmse_content, rmse_collab, rmse_hybrid]\n",
        "})\n",
        "\n",
        "print(\"📊 Evaluation Results:\")\n",
        "display(results_df)\n",
        "\n",
        "# Cell 8: Visualization\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x=\"System\", y=\"RMSE\", data=results_df, palette=\"viridis\")\n",
        "plt.title(\"RMSE Comparison (Lower is Better)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x=\"System\", y=\"MSE\", data=results_df, palette=\"magma\")\n",
        "plt.title(\"MSE Comparison (Lower is Better)\")\n",
        "plt.show()\n",
        "\n",
        "# Cell 9: Demo recommendations\n",
        "def demo_recommendations(movies, vectors, user_item_matrix, user_sim_df):\n",
        "    print(\"=== MOVIE RECOMMENDER SYSTEM DEMO ===\")\n",
        "\n",
        "    # Content-based recommendation demo\n",
        "    print(\"\\n1. Content-Based Recommendations:\")\n",
        "    movie_title = \"The Dark Knight\"  # Example movie\n",
        "    if movie_title in movies['title'].values:\n",
        "        method_name, recs = content_based_recommend(movie_title, movies, vectors, top_n=5)\n",
        "        print(f\"Top 5 content-based recommendations for '{movie_title}':\")\n",
        "        for i, (title, score) in enumerate(recs, 1):\n",
        "            print(f\"{i}. {title} (score: {score:.4f})\")\n",
        "    else:\n",
        "        print(f\"Movie '{movie_title}' not found in database\")\n",
        "\n",
        "    # Hybrid recommendation demo\n",
        "    print(\"\\n2. Hybrid Recommendations:\")\n",
        "    user_name = \"Bob\"\n",
        "    movie_title = \"The Dark Knight\"\n",
        "    if movie_title in movies['title'].values and user_name in user_item_matrix.index:\n",
        "        user_display, recs = hybrid_recommend(user_name, movie_title, movies, vectors, user_item_matrix, user_sim_df, top_n=5)\n",
        "        print(f\"Top 5 hybrid recommendations for user '{user_display}' based on '{movie_title}':\")\n",
        "        for i, (title, score) in enumerate(recs, 1):\n",
        "            print(f\"{i}. {title} (score: {score:.4f})\")\n",
        "    else:\n",
        "        print(f\"Movie '{movie_title}' or user '{user_name}' not found\")\n",
        "\n",
        "# Prepare data for demo recommendations\n",
        "ratings_demo, user_item_matrix_demo, user_sim_df_demo = prepare_collaborative_data(movies, ratings) # Use original ratings for demo\n",
        "\n",
        "# Run the demo\n",
        "demo_recommendations(movies, vectors, user_item_matrix_demo, user_sim_df_demo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "mfolsd7KviRN",
        "outputId": "69fcfb6e-d168-4bad-c95c-58f48cdbc1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2957222789.py:7: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  movies = pd.read_csv(movies_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/AI/credits.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3681573115.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load data and create TF-IDF model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmovies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_from_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tfidf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2957222789.py\u001b[0m in \u001b[0;36mload_data_from_drive\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcredits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredits_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/AI/credits.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    st.title(\"🎬 Movie Recommender System\")\n",
        "\n",
        "    # Load data and models\n",
        "    movies, ratings = load_data()\n",
        "    tfidf, vectors = create_tfidf_model(movies)\n",
        "    ratings, user_item_matrix, user_sim_df = prepare_collaborative_data(movies, ratings)\n",
        "\n",
        "    st.header(\"Movie Recommendations\")\n",
        "\n",
        "    rec_type = st.radio(\"Recommendation Type\", [\"Content-Based\", \"Hybrid\"])\n",
        "    user_choices = [\"-\", \"Bob\", \"Alice\", \"Charlie\", \"Diana\", \"Eve\"]\n",
        "    user_choice = st.selectbox(\"Select Movie Critic\", user_choices) if rec_type==\"Hybrid\" else \"-\"\n",
        "\n",
        "    movie_choices = sorted(movies['title'].unique())\n",
        "    movie_title = st.selectbox(\"Select a Movie\", movie_choices)\n",
        "    num_recs = st.slider(\"Number of Recommendations\", 1, 20, 10)\n",
        "\n",
        "    if st.button(\"Get Recommendations\"):\n",
        "        if not movie_title.strip():\n",
        "            st.error(\"❌ Please select a movie first.\")\n",
        "            return\n",
        "        if rec_type==\"Content-Based\":\n",
        "            method_name, recs = content_based_recommend(movie_title, movies, vectors, num_recs)\n",
        "            user_display = \"Content-Based Filtering\"\n",
        "        else:\n",
        "            user_input = \"\" if user_choice==\"-\" else user_choice\n",
        "            user_display, recs = hybrid_recommend(user_input, movie_title, movies, vectors, user_item_matrix, user_sim_df, top_n=num_recs)\n",
        "            method_name = \"Hybrid recommendations\"\n",
        "\n",
        "        if isinstance(recs,str) or len(recs)==0:\n",
        "            st.error(recs if isinstance(recs,str) else \"❌ No recommendations found\")\n",
        "        else:\n",
        "            max_score = max([s for _,s in recs]) if recs else 1.0\n",
        "            rows = [[f\"{i}. {title}\", f\"{(score/max_score)*100:.1f}%\"] for i,(title,score) in enumerate(recs,start=1)]\n",
        "            st.subheader(f\"🎭 {method_name} for {user_display} (based on {movie_title}):\")\n",
        "            st.table(pd.DataFrame(rows, columns=[\"Movie\",\"Score\"]))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqUv0eFWs0kh",
        "outputId": "d42bad25-9a34-4708-f926-1cccc9450ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-07 15:20:15.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.689 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.736 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.737 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.739 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.740 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.741 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.743 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.744 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.745 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.747 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.748 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.752 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.754 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.756 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.758 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.759 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.760 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.761 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.762 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.763 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.764 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.765 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.767 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.768 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.768 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.770 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.770 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-07 15:20:15.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded TF-IDF model from pickle\n"
          ]
        }
      ]
    }
  ]
}